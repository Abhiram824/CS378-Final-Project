{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DbqKgqWiVsz"
      },
      "source": [
        "## Problem Introduction: Safe Reinforcement Learning\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/eleurent/highway-env/gh-media/docs/media/roundabout-env.gif\"></img>\n",
        "\n",
        "\n",
        "**Optimal Planning with Oracle Model**\n",
        "When planning with the true model, an optimal policy generally operates as close as possible to the system constraints, resulting in dangerous behaviours.\n",
        "\n",
        "Example of unsafe behaviour:\n",
        "<a href=\"https://imgur.com/o5JvJXi\"><img src=\"https://i.imgur.com/o5JvJXi.png\" title=\"source: imgur.com\" /></a>\n",
        "\n",
        "When turning the green car takes a right but does not stay on the extreme right of the road. It follows the policy to not get hit but this is not safe since other cars may not behave optimally always.\n",
        "\n",
        "As an effect, even slight model errors can lead to catastrophic failures\n",
        "\n",
        "<a href=\"https://imgur.com/sIkfUCC\"><img src=\"https://i.imgur.com/sIkfUCC.png\" title=\"source: imgur.com\" /></a>\n",
        "  \n",
        "In order to account for model uncertainty, we follow the robust control framework:\n",
        "\n",
        "\"*Maximise the worst-case performance with respect to a set of possible behavioral models*\"\n",
        "\n",
        "**Robust Planning with Discrete Ambiguity**\n",
        "\n",
        "You can plan by considering every possible direction the traffic participants can take at their next intersection.\n",
        "\n",
        "\n",
        "**Reducing uncertainty of the next action of other cars**\n",
        "\n",
        "Let us discuss the reasons behind the uncertainty\n",
        "\n",
        "1. Driver on the road is not an optimal driver.\n",
        "2. We do not know the exact location of the driver.\n",
        "\n",
        "<!-- **Handling Scenario 1:**\n",
        "We plan by considering every possible direction that traffic participants can take at their next intersection.\n",
        "\n",
        "$\\mu$- mean path of each participant.\n",
        "\n",
        "<a href=\"https://imgur.com/hARWjSD\"><img src=\"https://i.imgur.com/hARWjSD.png\" title=\"source: imgur.com\" /></a>\n",
        "\n",
        "Continuous range of trajectories (variance over) path trajectories of each traffic participant based on their driving style.\n",
        "\n",
        "<a href=\"https://imgur.com/qd3kyZ0\"><img src=\"https://i.imgur.com/qd3kyZ0.png\" title=\"source: imgur.com\" /></a> -->\n",
        "\n",
        "To summarise, if everyone has the same driving behaviour we can assume drivers to behave as if they are driving on the optimal path. If we also think about each one having a driving behaviour i.e (some can be drunk, talking on phone, angry) then we get varying levels of uncertainty around the optimal path for each vehicle. This is accurately characterized below.\n",
        "\n",
        "<a href=\"https://imgur.com/qfDGiTx\"><img src=\"https://i.imgur.com/qfDGiTx.png\" title=\"source: imgur.com\" /></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eeje4O8fviH"
      },
      "source": [
        "# Model-Based Reinforcement Learning\n",
        "\n",
        "We first demonstrate a Model based approach and its failure cases and then moivate you to explore inverse reinforcement learning approaches. You can choose either of the above approaches.\n",
        "\n",
        "## Principle for Model based RL\n",
        "We consider the optimal control problem of an MDP with a **known** reward function $R$ and subject to **unknown deterministic** dynamics $s_{t+1} = f(s_t, a_t)$:\n",
        "\n",
        "$$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t R(s_t,a_t)$$\n",
        "\n",
        "In **model-based reinforcement learning**, this problem is solved in **two steps**:\n",
        "1. **Model learning**:\n",
        "We learn a model of the dynamics $f_\\theta \\simeq f$ through regression on interaction data.\n",
        "2. **Planning**:\n",
        "We leverage the dynamics model $f_\\theta$ to compute the optimal trajectory $$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t R(\\hat{s}_t,a_t)$$ following the learnt dynamics $\\hat{s}_{t+1} = f_\\theta(\\hat{s}_t, a_t)$.\n",
        "\n",
        "(We can easily extend to unknown rewards and stochastic dynamics, but we consider the simpler case in this notebook for ease of presentation)\n",
        "\n",
        "\n",
        "## Motivation\n",
        "\n",
        "### Sparse rewards\n",
        "* In model-free reinforcement learning, we only obtain a reinforcement signal when encountering rewards. In environment with **sparse rewards**, the chance of obtaining a reward randomly is **negligible**, which prevents any learning.\n",
        "* However, even in the **absence of rewards** we still receive a **stream of state transition data**. We can exploit this data to learn about the task at hand.\n",
        "\n",
        "### Complexity of the policy/value vs dynamics:\n",
        "Is it easier to decide which action is best, or to predict what is going to happen?\n",
        "* Some problems can have **complex dynamics** but a **simple optimal policy or value function**. For instance, consider the problem of learning to swim. Predicting the movement requires understanding fluid dynamics and vortices while the optimal policy simply consists in moving the limbs in sync.\n",
        "* Conversely, other problems can have **simple dynamics** but **complex policies/value functions**. Think of the game of Go, its rules are simplistic (placing a stone merely changes the board state at this location) but the corresponding optimal policy is very complicated.\n",
        "\n",
        "Intuitively, model-free RL should be applied to the first category of problems and model-based RL to the second category.\n",
        "\n",
        "### Inductive bias\n",
        "Oftentimes, real-world problems exhibit a particular **structure**: for instance, any problem involving motion of physical objects will be **continuous**. It can also be **smooth**, **invariant** to translations, etc. This knowledge can then be incorporated in machine learning models to foster efficient learning. In contrast, there can often be **discontinuities** in the policy decisions or value function: e.g. think of a collision vs near-collision state.\n",
        "\n",
        "###  Sample efficiency\n",
        "Overall, it is generally recognized that model-based approaches tend to **learn faster** than model-free techniques (see e.g. [[Sutton, 1990]](http://papersdb.cs.ualberta.ca/~papersdb/uploaded_files/paper_p160-sutton.pdf.stjohn)).\n",
        "\n",
        "### Interpretability\n",
        "In real-world applications, we may want to know **how a policy will behave before actually executing it**, for instance for **safety-check** purposes. However, model-free reinforcement learning only recommends which action to take at current time without being able to predict its consequences. In order to obtain the trajectory, we have no choice but executing the policy. In stark contrast, model-based methods a more interpretable in the sense that we can probe the policy for its intended (and predicted) trajectory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-oVNY_KTw6R"
      },
      "source": [
        "## Our challenge: Robust\n",
        "\n",
        "We consider the **roundabout-v0** task of the [highway-env](https://github.com/eleurent/highway-env) environment. It is a **continuous control** task where an agent **drives a car** by controlling the gaz pedal and steering angle and must **navigate safely** with the appropriate heading.\n",
        "\n",
        "\n",
        "\n",
        "###  Warming up\n",
        "We start with a few useful installs and imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzMSuJEOfviP"
      },
      "outputs": [],
      "source": [
        "# Install environment and visualization dependencies\n",
        "!pip install highway-env\n",
        "!pip install stable-baselines3\n",
        "!pip install d3rlpy\n",
        "!pip install sb3-contrib\n",
        "\n",
        "\n",
        "# Environment\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "\n",
        "# Models and computation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "# IO\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Bu_Pqop0E7"
      },
      "source": [
        "We also define a simple helper function for visualization of episodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so7yH4ucyB-3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm.notebook import trange\n",
        "!pip install tensorboardx gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb ffmpeg\n",
        "!git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8rJG4jzys8L"
      },
      "outputs": [],
      "source": [
        "%cd HighwayEnv/scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6gF3lvIyhnZ"
      },
      "outputs": [],
      "source": [
        "from utils import record_videos, show_videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFtBY6JSqPFa"
      },
      "source": [
        "### Let's try it!\n",
        "\n",
        "Make the environment, and run an episode with random actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKZt9Cb1rJ6n"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"roundabout-v0\", render_mode=\"rgb_array\")\n",
        "env = record_videos(env)\n",
        "env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "env.close()\n",
        "show_videos()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewG5f_essAS0"
      },
      "source": [
        "The environment is a `GoalEnv`, which means the agent receives a dictionary containing both the current `observation` and the `desired_goal` that conditions its policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIC98mGhr7v6"
      },
      "outputs": [],
      "source": [
        "print(\"Observation format:\", obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voagCILztJ3J"
      },
      "source": [
        "There is also an `achieved_goal` that won't be useful here (it only serves when the state and goal spaces are different, as a projection from the observation to the goal space).\n",
        "\n",
        "Alright! We are now ready to apply the inverse reinforcement learning paradigm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2PuVAvyfvib"
      },
      "source": [
        "## Experience collection\n",
        "First, we randomly interact with the environment to produce a batch of experiences\n",
        "\n",
        "$$D = \\{s_t, a_t, s_{t+1}\\}_{t\\in[1,N]}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbDTosZczGdE"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ['state', 'action', 'next_state'])\n",
        "\n",
        "def collect_interaction_data(env, size=1000, action_repeat=2):\n",
        "    data, done = [], True\n",
        "    for _ in trange(size, desc=\"Collecting interaction data\"):\n",
        "        action = env.action_space.sample()\n",
        "        for _ in range(action_repeat):\n",
        "            if done:\n",
        "              previous_obs, info = env.reset()\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "            # print(\"obs:\", obs)\n",
        "            # print(\"reward:\", reward)\n",
        "            # print(\"previous_obs:\", previous_obs)\n",
        "            # break\n",
        "            data.append(Transition(torch.Tensor(previous_obs),\n",
        "                                   torch.Tensor(action),\n",
        "                                   torch.Tensor(obs)))\n",
        "            previous_obs = obs\n",
        "        # break\n",
        "    return data\n",
        "\n",
        "env = gym.make(\"roundabout-v0\")\n",
        "data = collect_interaction_data(env)\n",
        "print(\"Sample transition:\", data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ7ZtBim6GYX"
      },
      "outputs": [],
      "source": [
        "! pip install --user git+https://github.com/shubham0704/rl-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHyDKIRt9Z3I"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/shubham0704/rl-agents.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuhUW72u5b-P"
      },
      "outputs": [],
      "source": [
        "%cd rl-agents/scripts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv0WJlA_5pVd"
      },
      "outputs": [],
      "source": [
        "! python experiments.py benchmark configs/RoundaboutEnv/benchmark_robust_control.json \\\n",
        "                      --test --episodes=100 --processes=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Th1JezEfvir"
      },
      "source": [
        "## Build a dynamics model\n",
        "\n",
        "Dynamics model depends on the nature of environment. Model based RL suffers from the problem of **Model bias**\n",
        "\n",
        "The model will be accurate only on some region of the state space that was explored and covered in  $D$ . Outside of  $D$ , the model may diverge and hallucinate important rewards. This effect is problematic when the model is used by a planning algorithm, as the latter will try to exploit these hallucinated high rewards and will steer the agent towards unknown (and thus dangerous) regions where the model is erroneously optimistic.\n",
        "\n",
        "You need to choose from the following:\n",
        "1. Model-based RL methods\n",
        "2. Model-free RL methods\n",
        "\n",
        "## Part 1\n",
        "\n",
        "List all the known methods you have studied in the class from 1 and 2 and suggest which is the best method you wish to implement for this problem.\n",
        "\n",
        "\n",
        "1. What is the method? Model-based or Model-free\n",
        "2. Motivation for choosing the method\n",
        "3. Problem formulation using your proposed method. Mention metrics (cost function), block-diagram of your pipeline with proper notation and mathematical formulation\n",
        "\n",
        "\n",
        "## Part 2\n",
        "\n",
        "Show implementation of your approach and mention results with plots. Add a section to showcase failure cases and mention limitation and future work.\n",
        "\n",
        "\n",
        "**Example of a dynamics model**\n",
        "\n",
        "Here is an sample example that uses a model based approach.\n",
        "\n",
        "We now design a model to represent the system dynamics. We choose  a **structured model** inspired from *Linear Time-Invariant (LTI) systems*\n",
        "\n",
        "$$\\dot{x} = f_\\theta(x, u) = A_\\theta(x, u)x + B_\\theta(x, u)u$$\n",
        "\n",
        "where the $(x, u)$ notation comes from the Control Theory community and stands for the state and action $(s,a)$. Intuitively, we learn at each point $(x_t, u_t)$ the **linearization** of the true dynamics $f$ with respect to $(x, u)$.\n",
        "\n",
        "We parametrize $A_\\theta$ and $B_\\theta$ as two fully-connected networks with one hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9G94Zjx137d"
      },
      "outputs": [],
      "source": [
        "env.unwrapped.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7Gl2kKJfviu"
      },
      "outputs": [],
      "source": [
        "class DynamicsModel(nn.Module):\n",
        "    STATE_X = 0\n",
        "    STATE_Y = 1\n",
        "\n",
        "    def __init__(self, state_size, action_size, hidden_size, dt):\n",
        "        super().__init__()\n",
        "        self.state_size, self.action_size, self.dt = state_size, action_size, dt\n",
        "        A_size, B_size = state_size * state_size, state_size * action_size\n",
        "        self.A1 = nn.Linear(state_size + action_size, hidden_size)\n",
        "        self.A2 = nn.Linear(hidden_size, A_size)\n",
        "        self.B1 = nn.Linear(state_size + action_size, hidden_size)\n",
        "        self.B2 = nn.Linear(hidden_size, B_size)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        \"\"\"\n",
        "            Predict x_{t+1} = f(x_t, u_t)\n",
        "        :param x: a batch of states\n",
        "        :param u: a batch of actions\n",
        "        \"\"\"\n",
        "        print(x.shape, u.shape)\n",
        "        xu = torch.cat((x, u), -1)\n",
        "        xu[:, self.STATE_X:self.STATE_Y+1] = 0  # Remove dependency in (x,y)\n",
        "        A = self.A2(F.relu(self.A1(xu)))\n",
        "        A = torch.reshape(A, (x.shape[0], self.state_size, self.state_size))\n",
        "        B = self.B2(F.relu(self.B1(xu)))\n",
        "        B = torch.reshape(B, (x.shape[0], self.state_size, self.action_size))\n",
        "        dx = A @ x.unsqueeze(-1) + B @ u.unsqueeze(-1)\n",
        "        return x + dx.squeeze()*self.dt\n",
        "\n",
        "# dynamics = DynamicsModel(state_size=env.observation_space.shape[0],\n",
        "#                          action_size=env.action_space.n,\n",
        "#                          hidden_size=64,\n",
        "#                          dt=1/env.unwrapped.config[\"policy_frequency\"])\n",
        "# print(\"Forward initial model on a sample transition:\", dynamics(data[0].state.unsqueeze(0),\n",
        "#                                                                 data[0].action.unsqueeze(0)).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5D6W4p7fvjI"
      },
      "source": [
        "## Scenario1: Leverage dynamics model for planning\n",
        "\n",
        "We now use the learnt dynamics model $f_\\theta$ for planning.\n",
        "In order to solve the optimal control problem, we use a sampling-based optimization algorithm: the **Cross-Entropy Method** (`CEM`). It is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions.\n",
        "\n",
        "This method approximates the optimal importance sampling estimator by repeating two phases:\n",
        "1. **Draw samples** from a probability distribution. We use Gaussian distributions over sequences of actions.\n",
        "2. Minimize the **cross-entropy** between this distribution and a **target distribution** to produce a better sample in the next iteration. We define this target distribution by selecting the top-k performing sampled sequences.\n",
        "\n",
        "![Credits to Olivier Sigaud](https://github.com/yfletberliac/rlss2019-hands-on/blob/master/imgs/cem.png?raw=1)\n",
        "\n",
        "Note that as we have a local linear dynamics model, we could instead choose an `Iterative LQR` planner which would be more efficient. We prefer `CEM` in this educational setting for its simplicity and generality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8L6vEPWyea7"
      },
      "source": [
        "## Visualize a few episodes\n",
        "\n",
        "En voiture, Simone !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOcOP7Of18T2"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"roundabout-v0\", render_mode='rgb_array')\n",
        "env = record_videos(env)\n",
        "obs, info = env.reset()\n",
        "\n",
        "for step in trange(3 * env.config[\"duration\"], desc=\"Testing 3 episodes...\"):\n",
        "    action = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
        "                         torch.Tensor(obs[\"desired_goal\"]),\n",
        "                         env.action_space.shape[0])\n",
        "    obs, reward, done, truncated, info = env.step(action.numpy())\n",
        "    if done or truncated:\n",
        "        obs, info = env.reset()\n",
        "env.close()\n",
        "show_videos()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psBBQIv4fvjT"
      },
      "source": [
        "# Mention Limitations of your approach\n",
        "\n",
        "### Example: Computational cost of planning\n",
        "\n",
        "At test time, the planning step typically requires **sampling a lot of trajectories** to find a near-optimal candidate, wich may turn out to be very costly. This may be prohibitive in a high-frequency real-time setting. The **model-free** methods which directly recommend the best action are **much more efficient** in that regard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCvPIBtVzXKU"
      },
      "source": [
        "# Our Approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure environment to give image observations"
      ],
      "metadata": {
        "id": "bPRZKhZ-qGtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import d3rlpy\n",
        "\n",
        "from sb3_contrib import TRPO\n",
        "from d3rlpy.dataset import MDPDataset, ReplayBuffer\n",
        "from stable_baselines3 import DQN\n",
        "from d3rlpy.dataset.episode_generator import EpisodeGenerator\n",
        "from d3rlpy.dataset.buffers import InfiniteBuffer\n",
        "from d3rlpy.algos import DiscreteCQLConfig\n",
        "from d3rlpy.metrics import EnvironmentEvaluator\n",
        "from d3rlpy.algos import DQNConfig\n",
        "\n"
      ],
      "metadata": {
        "id": "qpjy_eRFqPVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q36xeFGrzzRz"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"roundabout-v0\", render_mode='rgb_array')\n",
        "config = {\n",
        "       \"observation\": {\n",
        "           \"type\": \"GrayscaleObservation\",\n",
        "           \"observation_shape\": (128, 64),\n",
        "           \"stack_size\": 4,\n",
        "           \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
        "           \"scaling\": 1.75,\n",
        "       },\n",
        "       \"policy_frequency\": 2\n",
        "   }\n",
        "env.configure(config)\n",
        "obs, info = env.reset()\n",
        "print(obs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN8HVvwq0UFe"
      },
      "outputs": [],
      "source": [
        "env.unwrapped.config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train our TRPO agent"
      ],
      "metadata": {
        "id": "3XsKhDLFqWbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TRPO('CnnPolicy', env,\n",
        "              policy_kwargs=dict(net_arch=[256, 256]),\n",
        "              learning_rate=5e-4,\n",
        "              batch_size=32,\n",
        "              gamma=0.8,\n",
        "              verbose=1,\n",
        "              tensorboard_log=\"roundabout_trpo/\")\n",
        "model.learn(40000, progress_bar=True)\n",
        "model.save(\"roundabout_trpo/model\")\n"
      ],
      "metadata": {
        "id": "LwPmOfmwOeEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5toQ9N4zbnP"
      },
      "source": [
        "Train DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6mizoAPzslM"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = DQN('CnnPolicy', env,\n",
        "              policy_kwargs=dict(net_arch=[256, 256]),\n",
        "              learning_rate=5e-4,\n",
        "              buffer_size=15000,\n",
        "              learning_starts=200,\n",
        "              batch_size=32,\n",
        "              gamma=0.8,\n",
        "              train_freq=1,\n",
        "              gradient_steps=1,\n",
        "              target_update_interval=50,\n",
        "              verbose=1,\n",
        "              tensorboard_log=\"roundabout_dqn_final/\")\n",
        "model.learn(int(4e4))\n",
        "model.save(\"roundabout_dqn_final/model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize training results"
      ],
      "metadata": {
        "id": "aCKeSIH0qgnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5XybtJMdvrZ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeNOFg9Kr1OX"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/roundabout_trpo\" --port 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect demonstrations for offline learning\n",
        "\n"
      ],
      "metadata": {
        "id": "aRMX2ukIqoIV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNYuaLozjOb0"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = DQN.load(\"/content/roundabout_dqn_final/model\")\n",
        "episodes = []\n",
        "\n",
        "env = gym.make(\"roundabout-v0\", render_mode='rgb_array')\n",
        "config = {\n",
        "       \"observation\": {\n",
        "           \"type\": \"GrayscaleObservation\",\n",
        "           \"observation_shape\": (128, 64),\n",
        "           \"stack_size\": 4,\n",
        "           \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
        "           \"scaling\": 1.75,\n",
        "       }\n",
        "\n",
        "   }\n",
        "env.configure(config)\n",
        "\n",
        "\n",
        "eps = 0.8\n",
        "states = []\n",
        "actions = []\n",
        "rewards = []\n",
        "dones = []\n",
        "\n",
        "for _ in trange(1000):  # Number of episodes\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    steps = 0\n",
        "    while not done and steps < 10:\n",
        "        # sampled = random.uniform(0, 1)\n",
        "        # if sampled > eps:\n",
        "        #     action = env.action_space.sample()\n",
        "        # else:\n",
        "        action, _ = model.predict(state)\n",
        "\n",
        "\n",
        "        next_state, reward, done, truncated, info  = env.step(action)\n",
        "        # print(state.shape, action, type(reward), done)\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "\n",
        "        state = next_state\n",
        "        steps +=1\n",
        "\n",
        "states = np.array(states)\n",
        "actions = np.array(actions)\n",
        "rewards = np.array(rewards)\n",
        "dones = np.array(dones)\n",
        "\n",
        "episode_generator = EpisodeGenerator(\n",
        "            observations=states,\n",
        "            actions=actions,\n",
        "            rewards=rewards,\n",
        "            terminals=dones,\n",
        "            timeouts=None,\n",
        "        )\n",
        "buffer = InfiniteBuffer()\n",
        "\n",
        "# print(states.shape, actions.shape, rewards.shape, dones.shape)\n",
        "# Convert to MDPDataset\n",
        "\n",
        "dataset = ReplayBuffer(buffer,\n",
        "            episodes=episode_generator(), env=env\n",
        "        )\n",
        "with open(\"dataset.h5\", \"w+b\") as f:\n",
        "    dataset.dump(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE_TqZqBWQ_R"
      },
      "outputs": [],
      "source": [
        "import d3rlpy\n",
        "with open(\"dataset.h5\", \"rb\") as f:\n",
        "    dataset = d3rlpy.dataset.ReplayBuffer.load(f, d3rlpy.dataset.InfiniteBuffer())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train CQL offline"
      ],
      "metadata": {
        "id": "GQkRW-xGq5Bx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiqyUQQ7kUDy"
      },
      "outputs": [],
      "source": [
        "\n",
        "cql = d3rlpy.algos.DiscreteCQLConfig().create(device='cuda:0')\n",
        "# Train the algorithm on the dataset\n",
        "cql.fit(\n",
        "    dataset,\n",
        "    n_steps=10000,\n",
        "    n_steps_per_epoch=1000,\n",
        "    evaluators={\n",
        "        'environment': EnvironmentEvaluator(env)\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer q-function to DQN and finetune it online"
      ],
      "metadata": {
        "id": "vDbVyPy-rD92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf3nDKfrFXLp"
      },
      "outputs": [],
      "source": [
        "\n",
        "buffer = d3rlpy.dataset.create_fifo_replay_buffer(limit=100000, env=env)\n",
        "\n",
        "\n",
        "explorer = d3rlpy.algos.ConstantEpsilonGreedy(0.1)\n",
        "\n",
        "dqn = d3rlpy.algos.DQNConfig().create(device=\"cuda:0\")\n",
        "dqn.build_with_env(env)\n",
        "dqn.copy_q_function_from(cql)\n",
        "\n",
        "dqn.fit_online(env, buffer, explorer, n_steps=40000, n_steps_per_epoch=1000, update_start_step=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzSqBEdOnb3q"
      },
      "outputs": [],
      "source": [
        "%cd HighwayEnv/scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rollout evaluation. Replace the model variable with the relevant model we want to take a look at"
      ],
      "metadata": {
        "id": "yWNqhK_5rbmR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMxMrwiA2Q4_"
      },
      "outputs": [],
      "source": [
        "#replace this with relevant model for evaluation\n",
        "model = DQN.load(\"/content/model\")\n",
        "\n",
        "\n",
        "\n",
        "env = gym.make(\"roundabout-v0\", render_mode='rgb_array')\n",
        "config = {\n",
        "       \"observation\": {\n",
        "           \"type\": \"GrayscaleObservation\",\n",
        "           \"observation_shape\": (128, 64),\n",
        "           \"stack_size\": 4,\n",
        "           \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
        "           \"scaling\": 1.75,\n",
        "       },\n",
        "       \"policy_frequency\": 2\n",
        "   }\n",
        "env.configure(config)\n",
        "rewards_list = []\n",
        "# env = record_videos(env)\n",
        "for i in trange(50):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    total_rw = 0\n",
        "    #stop at 22 because configureation truncates at 11 seconds and there are 2 steps per second\n",
        "    while not done and steps < 22:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        steps +=1\n",
        "        total_rw += reward\n",
        "    rewards_list.append(total_rw)\n",
        "\n",
        "print(min(rewards_list))\n",
        "print(sum(rewards_list)/len(rewards_list))\n",
        "print(rewards_list)\n",
        "    # env.close()\n",
        "    # show_videos()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}